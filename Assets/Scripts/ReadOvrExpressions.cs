using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using TMPro;
using System.Data;
using System;
using Unity.VisualScripting.FullSerializer;

//Next time note can't run compounding factors on certain expressions likely invalid and max.


namespace Oculus.Movement.Effects {
    public class ReadOvrExpressions : MonoBehaviour
    {

        protected OVRFaceExpressions _ovrFaceExpressions;

        protected OVRFaceExpressions _calibratedNeutralExpression = new OVRFaceExpressions();

        public MacroFacialExpressionDetector _facialExpressionDetector;

        public TMP_Text text;

        private bool canprogress = false;

        private bool NextExpression = false;

        private OVRFaceExpressions.FaceExpression NoFacialExpression = OVRFaceExpressions.FaceExpression.Invalid;

        private bool calibrated = false;

        private Dictionary<OVRFaceExpressions.FaceExpression,
            List<OVRFaceExpressions.FaceExpression>> compoundingExpressions;

        // Start is called before the first frame update
        void Start()
        {
            text.text = "";
            StartCoroutine(Tutorial());
        }

        IEnumerator Tutorial()
        {
            yield return PrintInstructions("Welcome to the Facial Expression Experiment!", NoFacialExpression);
            yield return PrintInstructions(
                @"This experiment consists of testing different boundries for different 
                facial expressions. You will first be asked to calibrate each expression, then
                you will attempt to hold that expression at different levels of exaggeration.
                ", NoFacialExpression);
            yield return PrintInstructions(
                @"We will first begin by calibrating your face. Hold a neutral expression and once you are
                ready, press A to lock in a natural expression", NoFacialExpression
                );

            yield return PrintInstructions(
                @"You should now attempt to smirt with the right side of your face.
                ", OVRFaceExpressions.FaceExpression.LipCornerPullerR);
            LogCompoundingFacialExpressions();
        }

        //This function was autogenerated by github copilot
        void LogCompoundingFacialExpressions()
        {
              foreach (KeyValuePair<OVRFaceExpressions.FaceExpression, List<OVRFaceExpressions.FaceExpression>> entry in compoundingExpressions)
            {
                text.text += entry.Key.ToString() + ": ";
                foreach (OVRFaceExpressions.FaceExpression ex in entry.Value)
                {
                    text.text += ex.ToString() + ", ";
                }
                text.text += "\n";
            }
        }



        void Cycle()
        {
            text.text = "";
            _ovrFaceExpressions = _facialExpressionDetector.GetOVRExpression();
            if (!_ovrFaceExpressions.FaceTrackingEnabled ||
                !_ovrFaceExpressions.ValidExpressions)
            {
                return;
            }

            OVRFaceExpressions.FaceExpression[] expression = {
                OVRFaceExpressions.FaceExpression.BrowLowererL,
                OVRFaceExpressions.FaceExpression.BrowLowererR};

            foreach (OVRFaceExpressions.FaceExpression ex
                in expression)
            {
                var RoundedValue = System.Math.Round(_ovrFaceExpressions[ex], 2);
                text.text += ex.ToString() + ": " + RoundedValue.ToString() + "\n";
            }


        }

        private void CalibrateNeutralFacialExpression()
        {
            _calibratedNeutralExpression  = _facialExpressionDetector.GetOVRExpression();
        }



        //This takes in a message conveyed to the participant and a facial expression we are
        //attempting to evaluate
        //after the message has displayed for three seconds we allow the user to progress
        //The idea is that we can allow a user to decide where they 

        IEnumerator PrintInstructions(string InstructionMessage, OVRFaceExpressions.FaceExpression currentFacialExpression)
        {

            //This is you/ This is the expression you are trying to make
            // Attempt to showcase this with 2 models provided

            text.text = InstructionMessage;
            yield return new WaitForSeconds(3);
            canprogress = true;
            text.text += "\n\nPress A to continue";

            while (!NextExpression)
            {
                yield return null;
            }
            if (!calibrated)
            {
                CalibrateNeutralFacialExpression();
                calibrated = true;
            }
            if (currentFacialExpression != OVRFaceExpressions.FaceExpression.Invalid)
            {
                CompoundingFacialExpressionTracker(currentFacialExpression);
            }
            NextExpression = false;

        }


        private void CompoundingFacialExpressionTracker(OVRFaceExpressions.FaceExpression currentFacialExpression)
        {
            _ovrFaceExpressions = _facialExpressionDetector.GetOVRExpression();
            foreach (OVRFaceExpressions.FaceExpression ex
                in Enum.GetValues(typeof(OVRFaceExpressions.FaceExpression))) //iterates through the enums
            {
                if (_ovrFaceExpressions[ex] > _calibratedNeutralExpression[ex] + 0.2f && ex != currentFacialExpression)
                {
                    compoundingExpressions[currentFacialExpression].Add(ex);
                }

            }
        }



        //This updates a few flags so we can progress
        //when the user presses the a button

        private void Update()
        {
            if (canprogress && OVRInput.Get(OVRInput.Button.One))
            {
                NextExpression = true;
                canprogress = false;
            }
        }
    }
}